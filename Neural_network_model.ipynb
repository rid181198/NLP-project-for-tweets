{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+IEJ8H1upOq5YI2r0KN6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rid181198/NLP-project-for-tweets/blob/main/Neural_network_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the necessary libraries"
      ],
      "metadata": {
        "id": "Gv0zHAlOKnok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "NFKTp-BEKGXX"
      },
      "outputs": [],
      "source": [
        "#basic libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text processing libraries\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "#stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from string import punctuation\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AusneSsVKh43",
        "outputId": "9c322ad5-01c0-48e1-b950-00c7912e1a7b"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the training dataset"
      ],
      "metadata": {
        "id": "bCeVUvegKtxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the dataset of all tweets is loaded and blank fields are filled with the empty string. Also, two other columns *tokenized_text* and *tokenized_key* are created to store the tokenized words for both text sentence of the tweet and the keywords, respectively."
      ],
      "metadata": {
        "id": "VlT4fW1PKv1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('./train.csv')\n",
        "data.fillna('', inplace=True)\n",
        "\n",
        "data['tokenized_text'] = \" \"\n",
        "data['tokenized_key'] = \" \""
      ],
      "metadata": {
        "id": "Ogemu8JsKlV8"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the STOPWORDS"
      ],
      "metadata": {
        "id": "fV1_0OyXLBo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*stop_words*, *punctuation*, *gensimwords*, *sklearnwords*, *num_pattern* are regarded as general words like she, he, I will be ignored. Also, *punctuation* referred for the punctuations like \",?!#@, etc. *gensimwords* and *sklearnwords* are the extended version of words like nonetheless, although, otherwise, etc. *num_pattern* finds the numbers and replaces with the empty string."
      ],
      "metadata": {
        "id": "_32oTyiELCc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = list(punctuation)\n",
        "gensimwords = STOPWORDS\n",
        "sklearnwords = ENGLISH_STOP_WORDS\n",
        "num_pattern = r'[0-9]'"
      ],
      "metadata": {
        "id": "F69a4CKmLFMx"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to tokenize/split the words and ignore the STOPWORDS"
      ],
      "metadata": {
        "id": "Hy426kimLHpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here *PorterStemmer* is an object to get the root or basis of words. For instance, playing will be considered as play, cats as cat. Using the functional library *word_tokenize*, the sentence is converted into the tokens or split words and afterwards, the STOPWORDS and stemming are done."
      ],
      "metadata": {
        "id": "cIfc39PLLJPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "porter = PorterStemmer()\n",
        "def tokenized_stop(string):\n",
        "    string = re.sub(num_pattern, '', string)\n",
        "    string = re.sub(r'http\\S+', '', string)\n",
        "    \n",
        "    #tokenizing the words\n",
        "    string = word_tokenize(string)\n",
        "    \n",
        "    #ignoring the unnecessary words\n",
        "    string_list = []\n",
        "    for words in string:\n",
        "        words = words.casefold()\n",
        "        if (words in stop_words) or (words in punctuation) or (words in gensimwords) or (words in sklearnwords):\n",
        "            pass\n",
        "        else:\n",
        "            words = porter.stem(words)\n",
        "            string_list.append(words)\n",
        "             \n",
        "    return string_list\n",
        "\n"
      ],
      "metadata": {
        "id": "EZ7hUtEJLLMY"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing the tokenized sentence and keywords to newly created columns"
      ],
      "metadata": {
        "id": "cM9Lh26WLNeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data['text'])):\n",
        "    data.at[i,'tokenized_text'] = tokenized_stop(data['text'][i])\n",
        "    data.at[i,'tokenized_key'] = tokenized_stop(data['keyword'][i])"
      ],
      "metadata": {
        "id": "xVhRwdzMLOww"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining all the sentences in a list for the categorical features assignment"
      ],
      "metadata": {
        "id": "zxoVk55uL7f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_sents=[]\n",
        "for i in range(len(data['tokenized_text'])):\n",
        "    string=''\n",
        "    for j in data['tokenized_text'][i]:\n",
        "        string = string  + j + ' '\n",
        "    all_sents.append(string)\n"
      ],
      "metadata": {
        "id": "xlIQp9P8L8wj"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the preprocessing tokenizer and sequencing to the sentences using *keras*"
      ],
      "metadata": {
        "id": "-OSmAxjkMBfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sents)\n",
        "sequenced_sents = tokenizer.texts_to_sequences(all_sents)"
      ],
      "metadata": {
        "id": "WDsK2yTFL_qg"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length_sent=[]\n",
        "for i in sequenced_sents:\n",
        "  length_sent.append(len(i))\n",
        "\n",
        "maxlen=max(length_sent)\n",
        "print(maxlen)\n",
        "\n",
        "total_vocab =[]\n",
        "for i in sequenced_sents:\n",
        "  for j in i:\n",
        "    total_vocab.append(j)\n",
        "\n",
        "vocab_size = len(set(total_vocab))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvcpMKJZOTtH",
        "outputId": "198e9661-94c8-42dd-cee6-bef4e85eb4d3"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "13958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding the sentence to make every sentence with equal length of words"
      ],
      "metadata": {
        "id": "JeWGl2PWN2IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequence = pad_sequences(sequenced_sents, maxlen=23)\n",
        "y_train = data['target']"
      ],
      "metadata": {
        "id": "hjplpHZZN1F9"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network model using the embedding and dense layers"
      ],
      "metadata": {
        "id": "FI6QzdaBSawe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size+1, 256, input_length=23))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "njmDw98RSheK"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocab_size+1, 256, input_length=23))\n",
        "model2.add(Conv1D(256, 3, activation='relu'))\n",
        "model2.add(MaxPooling1D(5))\n",
        "model2.add(Dense(128, activation='relu'))\n",
        "model2.add(Dense(32, activation='relu'))\n",
        "model2.add(GlobalMaxPooling1D())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "hrJuczmMVzsQ"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#model2.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "NuF6ovpsTDqz"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vals = model2.fit(padded_sequence,y_train,epochs=10, batch_size=16, validation_split=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew-mxru5XThb",
        "outputId": "386f109c-b586-43e5-93aa-46eecb0acdf0"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "334/334 [==============================] - 21s 59ms/step - loss: 0.5585 - accuracy: 0.7155 - val_loss: 0.5372 - val_accuracy: 0.7548\n",
            "Epoch 2/10\n",
            "334/334 [==============================] - 19s 58ms/step - loss: 0.3006 - accuracy: 0.8840 - val_loss: 0.6059 - val_accuracy: 0.7351\n",
            "Epoch 3/10\n",
            "334/334 [==============================] - 20s 58ms/step - loss: 0.1498 - accuracy: 0.9512 - val_loss: 0.7851 - val_accuracy: 0.7128\n",
            "Epoch 4/10\n",
            "334/334 [==============================] - 20s 60ms/step - loss: 0.0891 - accuracy: 0.9690 - val_loss: 0.8995 - val_accuracy: 0.7259\n",
            "Epoch 5/10\n",
            "334/334 [==============================] - 20s 59ms/step - loss: 0.0657 - accuracy: 0.9762 - val_loss: 0.8948 - val_accuracy: 0.7255\n",
            "Epoch 6/10\n",
            "334/334 [==============================] - 19s 58ms/step - loss: 0.0538 - accuracy: 0.9795 - val_loss: 0.9000 - val_accuracy: 0.7145\n",
            "Epoch 7/10\n",
            "334/334 [==============================] - 19s 58ms/step - loss: 0.0420 - accuracy: 0.9812 - val_loss: 1.2184 - val_accuracy: 0.7220\n",
            "Epoch 8/10\n",
            "334/334 [==============================] - 22s 66ms/step - loss: 0.0395 - accuracy: 0.9818 - val_loss: 1.1825 - val_accuracy: 0.7250\n",
            "Epoch 9/10\n",
            "334/334 [==============================] - 20s 59ms/step - loss: 0.0352 - accuracy: 0.9803 - val_loss: 1.1859 - val_accuracy: 0.7185\n",
            "Epoch 10/10\n",
            "334/334 [==============================] - 20s 59ms/step - loss: 0.0335 - accuracy: 0.9825 - val_loss: 1.4360 - val_accuracy: 0.7084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('./test.csv')\n",
        "test_data.fillna('', inplace=True)\n",
        "\n",
        "test_data['tokenized_text'] = \" \"\n",
        "test_data['tokenized_key'] = \" \"\n"
      ],
      "metadata": {
        "id": "ZrxBS8dacz8M"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test_data['text'])):\n",
        "    test_data.at[i,'tokenized_text'] = tokenized_stop(test_data['text'][i])\n",
        "    test_data.at[i,'tokenized_key'] = tokenized_stop(test_data['keyword'][i])"
      ],
      "metadata": {
        "id": "hdimDzrwc273"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sents=[]\n",
        "for i in range(len(test_data['tokenized_text'])):\n",
        "    string=''\n",
        "    for j in test_data['tokenized_text'][i]:\n",
        "        string = string  + j + ' '\n",
        "    all_sents.append(string)"
      ],
      "metadata": {
        "id": "GZ6k7gMwc8PL"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sents)\n",
        "sequenced_sents = tokenizer.texts_to_sequences(all_sents)"
      ],
      "metadata": {
        "id": "lXd6E5UKdjzE"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length_sent=[]\n",
        "for i in sequenced_sents:\n",
        "  length_sent.append(len(i))\n",
        "\n",
        "maxlen=max(length_sent)\n",
        "print(maxlen)\n",
        "\n",
        "total_vocab =[]\n",
        "for i in sequenced_sents:\n",
        "  for j in i:\n",
        "    total_vocab.append(j)\n",
        "\n",
        "vocab_size = len(set(total_vocab))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ1B4Xt5c-dX",
        "outputId": "752015ee-ced4-4187-8754-94e44d395b34"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n",
            "8377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "test_padded_sequence = pad_sequences(sequenced_sents, maxlen=maxlen)\n"
      ],
      "metadata": {
        "id": "c8p4tArNdAOq"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.predict(test_padded_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqglgPCmd60e",
        "outputId": "4c8b6712-2fe0-4ab8-c9b8-4e0d8bc90a05"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102/102 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=[]\n",
        "for i in results:\n",
        "  if i<0.5:\n",
        "    predictions.append(0)\n",
        "  if i>=0.5:\n",
        "    predictions.append(1)"
      ],
      "metadata": {
        "id": "ORQguixAke4P"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = test_data['id']\n",
        "\n",
        "results2 = {'id':ids,'target':predictions}\n",
        "results2 = pd.DataFrame(results2)\n",
        "results2.to_csv('./results2.csv')\n"
      ],
      "metadata": {
        "id": "ukRGytsZqvjA"
      },
      "execution_count": 146,
      "outputs": []
    }
  ]
}