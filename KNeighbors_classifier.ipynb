{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe57ae4d",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfcdd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import math\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1563bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text processing libraries\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fb3f6",
   "metadata": {},
   "source": [
    "## Loading the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e74ba",
   "metadata": {},
   "source": [
    "Here the dataset of all tweets is loaded and blank fields are filled with the empty string. Also, two other columns *tokenized_text* and *tokenized_key* are created to store the tokenized words for both text sentence of the tweet and the keywords, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "577048a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./train.csv')\n",
    "data.fillna('', inplace=True)\n",
    "\n",
    "data['tokenized_text'] = \" \"\n",
    "data['tokenized_key'] = \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042deec",
   "metadata": {},
   "source": [
    "## Creating the STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c4994",
   "metadata": {},
   "source": [
    "*stop_words*, *punctuation*, *gensimwords*, *sklearnwords*, *num_pattern* are regarded as general words like she, he, I will be ignored. Also, *punctuation* referred for the punctuations like \",?!#@, etc. *gensimwords* and *sklearnwords* are the extended version of words like nonetheless, although, otherwise, etc. *num_pattern* finds the numbers and replaces with the empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a651105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = list(punctuation)\n",
    "gensimwords = STOPWORDS\n",
    "sklearnwords = ENGLISH_STOP_WORDS\n",
    "num_pattern = r'[0-9]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9a21a",
   "metadata": {},
   "source": [
    "# Function to tokenize/split the words and ignore the STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096cf177",
   "metadata": {},
   "source": [
    "Here *PorterStemmer* is an object to get the root or basis of words. For instance, playing will be considered as play, cats as cat. Using the functional library *word_tokenize*, the sentence is converted into the tokens or split words and afterwards, the STOPWORDS and stemming are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d38a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenized_stop(string):\n",
    "    string = re.sub(num_pattern, '', string)\n",
    "    string = re.sub(r'http\\S+', '', string)\n",
    "    \n",
    "    #tokenizing the words\n",
    "    string = word_tokenize(string)\n",
    "    \n",
    "    #ignoring the unnecessary words\n",
    "    string_list = []\n",
    "    for words in string:\n",
    "        words = words.casefold()\n",
    "        if (words in stop_words) or (words in punctuation) or (words in gensimwords) or (words in sklearnwords):\n",
    "            pass\n",
    "        else:\n",
    "            words = porter.stem(words)\n",
    "            string_list.append(words)\n",
    "             \n",
    "    return string_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08396d13",
   "metadata": {},
   "source": [
    "## Storing the tokenized sentence and keywords to newly created columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a7d6873",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['text'])):\n",
    "    data.at[i,'tokenized_text'] = tokenized_stop(data['text'][i])\n",
    "    data.at[i,'tokenized_key'] = tokenized_stop(data['keyword'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78413013",
   "metadata": {},
   "source": [
    "## Combining all the sentences in a list for the categorical features assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1673b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents=[]\n",
    "for i in range(len(data['tokenized_text'])):\n",
    "    string=''\n",
    "    for j in data['tokenized_text'][i]:\n",
    "        string = string  + j + ' '\n",
    "    all_sents.append(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df256f80",
   "metadata": {},
   "source": [
    "## Example of assigning the numerics to the words using *sklearn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "918e41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "#vectorizer.fit(all_words)\n",
    "X_train_counts = vectorizer.fit_transform(all_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a7f5d9",
   "metadata": {},
   "source": [
    "## Example of converting the numerics into weighted average values using *sklearn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24332ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea99bbf",
   "metadata": {},
   "source": [
    "## Training input and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0be137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = all_sents\n",
    "y_train = data['target']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da13d83",
   "metadata": {},
   "source": [
    "## Pipeline of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67fd81",
   "metadata": {},
   "source": [
    "This pipeline contains categorical feature conversion and grid search model of KNeighbors classifier. The pipeline simplifies the long textual code into single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f6d87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\\\n",
    "                     ('tfidf', TfidfTransformer()),\\\n",
    "                     ('clf', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0862c3",
   "metadata": {},
   "source": [
    "Assigning the variables for all parameters given in the pipeline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49ac73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'tfidf__use_idf': (True, False),\\\n",
    "               'clf__weights': ('uniform','distance'),\\\n",
    "                'clf__algorithm': ('ball_tree','kd_tree','brute','auto'),\\\n",
    "             'clf__n_neighbors': (300,1000) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf24d6",
   "metadata": {},
   "source": [
    "## Running the KNeighbors pipeline for the training dataset using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "147d8b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rid/opt/anaconda3/lib/python3.8/site-packages/sklearn/neighbors/_base.py:462: UserWarning: cannot use tree with sparse input: using brute force\n",
      "  warnings.warn(\"cannot use tree with sparse input: \"\n"
     ]
    }
   ],
   "source": [
    "grid_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "grid_clf = grid_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f268e1e",
   "metadata": {},
   "source": [
    "## Best score of the training sample and best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46533792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6990759299156257\n",
      "{'clf__algorithm': 'ball_tree', 'clf__n_neighbors': 300, 'clf__weights': 'distance', 'tfidf__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "print(grid_clf.best_score_)\n",
    "print(grid_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b37f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c30316b",
   "metadata": {},
   "source": [
    "## Loading the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a94ec101",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./test.csv')\n",
    "test_data.fillna('', inplace=True)\n",
    "\n",
    "test_data['tokenized_text'] = \" \"\n",
    "test_data['tokenized_key'] = \" \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005a786",
   "metadata": {},
   "source": [
    "## Implementing the same text processing to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "56e7a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data['text'])):\n",
    "    test_data.at[i,'tokenized_text'] = tokenized_stop(test_data['text'][i])\n",
    "    test_data.at[i,'tokenized_key'] = tokenized_stop(test_data['keyword'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f817544",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents=[]\n",
    "for i in range(len(test_data['tokenized_text'])):\n",
    "    string=''\n",
    "    for j in test_data['tokenized_text'][i]:\n",
    "        string = string  + j + ' '\n",
    "    all_sents.append(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f086097",
   "metadata": {},
   "source": [
    "## Test data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8da635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = all_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f020b",
   "metadata": {},
   "source": [
    "## Prediction of the accidents or natural disasters from the textual tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24d9ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "predicted = grid_clf.predict(x_test)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21034e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
